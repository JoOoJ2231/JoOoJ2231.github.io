

### 프로젝트 목표
우리가 어떤 데이터를 받고 그를 분석한다면,우선적으로 분석에 대한 목표가 필요합니다. 그래야 어떤 방법론과 모델로 데이터에 접근할지 정할 수 있으니까요. 

#### 이번 반도체 공정 '데이터 분석'을 통해 다음 두 가지를 목표합니다.
1. '공정 이상을 예측' 하는 분류 모델을 생성합니다.
2.  데이터분석을 통해 '공정 이상에 영향을 미치는 요소들을 파악'합니다.

** 데이터의 출처: https://www.kaggle.com/paresh2047/uci-semcom/version/1
  UCI machine learning repository에서 참조해, 캐글에서 csv파일을 다운받았습니다.
위 데이터는 반도체 센서가 측정한 수치와 그에 따른 pass / fail 정보를 나타내는 데이터입니다.


```python
import pandas as pd

df = pd.read_csv("uci-secom.csv")
```

먼저 데이터가 어떤 구조로 되어있는지 봅시다. 


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>581</th>
      <th>582</th>
      <th>583</th>
      <th>584</th>
      <th>585</th>
      <th>586</th>
      <th>587</th>
      <th>588</th>
      <th>589</th>
      <th>Pass/Fail</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2008-07-19 11:55:00</td>
      <td>3030.93</td>
      <td>2564.00</td>
      <td>2187.7333</td>
      <td>1411.1265</td>
      <td>1.3602</td>
      <td>100.0</td>
      <td>97.6133</td>
      <td>0.1242</td>
      <td>1.5005</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.5005</td>
      <td>0.0118</td>
      <td>0.0035</td>
      <td>2.3630</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2008-07-19 12:32:00</td>
      <td>3095.78</td>
      <td>2465.14</td>
      <td>2230.4222</td>
      <td>1463.6606</td>
      <td>0.8294</td>
      <td>100.0</td>
      <td>102.3433</td>
      <td>0.1247</td>
      <td>1.4966</td>
      <td>...</td>
      <td>208.2045</td>
      <td>0.5019</td>
      <td>0.0223</td>
      <td>0.0055</td>
      <td>4.4447</td>
      <td>0.0096</td>
      <td>0.0201</td>
      <td>0.0060</td>
      <td>208.2045</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2008-07-19 13:17:00</td>
      <td>2932.61</td>
      <td>2559.94</td>
      <td>2186.4111</td>
      <td>1698.0172</td>
      <td>1.5102</td>
      <td>100.0</td>
      <td>95.4878</td>
      <td>0.1241</td>
      <td>1.4436</td>
      <td>...</td>
      <td>82.8602</td>
      <td>0.4958</td>
      <td>0.0157</td>
      <td>0.0039</td>
      <td>3.1745</td>
      <td>0.0584</td>
      <td>0.0484</td>
      <td>0.0148</td>
      <td>82.8602</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2008-07-19 14:43:00</td>
      <td>2988.72</td>
      <td>2479.90</td>
      <td>2199.0333</td>
      <td>909.7926</td>
      <td>1.3204</td>
      <td>100.0</td>
      <td>104.2367</td>
      <td>0.1217</td>
      <td>1.4882</td>
      <td>...</td>
      <td>73.8432</td>
      <td>0.4990</td>
      <td>0.0103</td>
      <td>0.0025</td>
      <td>2.0544</td>
      <td>0.0202</td>
      <td>0.0149</td>
      <td>0.0044</td>
      <td>73.8432</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2008-07-19 15:22:00</td>
      <td>3032.24</td>
      <td>2502.87</td>
      <td>2233.3667</td>
      <td>1326.5200</td>
      <td>1.5334</td>
      <td>100.0</td>
      <td>100.3967</td>
      <td>0.1235</td>
      <td>1.5031</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.4800</td>
      <td>0.4766</td>
      <td>0.1045</td>
      <td>99.3032</td>
      <td>0.0202</td>
      <td>0.0149</td>
      <td>0.0044</td>
      <td>73.8432</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 592 columns</p>
</div>




```python
df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1567 entries, 0 to 1566
    Columns: 592 entries, Time to Pass/Fail
    dtypes: float64(590), int64(1), object(1)
    memory usage: 7.1+ MB



```python
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>581</th>
      <th>582</th>
      <th>583</th>
      <th>584</th>
      <th>585</th>
      <th>586</th>
      <th>587</th>
      <th>588</th>
      <th>589</th>
      <th>Pass/Fail</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1562</th>
      <td>2008-10-16 15:13:00</td>
      <td>2899.41</td>
      <td>2464.36</td>
      <td>2179.7333</td>
      <td>3085.3781</td>
      <td>1.4843</td>
      <td>100.0</td>
      <td>82.2467</td>
      <td>0.1248</td>
      <td>1.3424</td>
      <td>...</td>
      <td>203.1720</td>
      <td>0.4988</td>
      <td>0.0143</td>
      <td>0.0039</td>
      <td>2.8669</td>
      <td>0.0068</td>
      <td>0.0138</td>
      <td>0.0047</td>
      <td>203.1720</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1563</th>
      <td>2008-10-16 20:49:00</td>
      <td>3052.31</td>
      <td>2522.55</td>
      <td>2198.5667</td>
      <td>1124.6595</td>
      <td>0.8763</td>
      <td>100.0</td>
      <td>98.4689</td>
      <td>0.1205</td>
      <td>1.4333</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.4975</td>
      <td>0.0131</td>
      <td>0.0036</td>
      <td>2.6238</td>
      <td>0.0068</td>
      <td>0.0138</td>
      <td>0.0047</td>
      <td>203.1720</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1564</th>
      <td>2008-10-17 05:26:00</td>
      <td>2978.81</td>
      <td>2379.78</td>
      <td>2206.3000</td>
      <td>1110.4967</td>
      <td>0.8236</td>
      <td>100.0</td>
      <td>99.4122</td>
      <td>0.1208</td>
      <td>NaN</td>
      <td>...</td>
      <td>43.5231</td>
      <td>0.4987</td>
      <td>0.0153</td>
      <td>0.0041</td>
      <td>3.0590</td>
      <td>0.0197</td>
      <td>0.0086</td>
      <td>0.0025</td>
      <td>43.5231</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1565</th>
      <td>2008-10-17 06:01:00</td>
      <td>2894.92</td>
      <td>2532.01</td>
      <td>2177.0333</td>
      <td>1183.7287</td>
      <td>1.5726</td>
      <td>100.0</td>
      <td>98.7978</td>
      <td>0.1213</td>
      <td>1.4622</td>
      <td>...</td>
      <td>93.4941</td>
      <td>0.5004</td>
      <td>0.0178</td>
      <td>0.0038</td>
      <td>3.5662</td>
      <td>0.0262</td>
      <td>0.0245</td>
      <td>0.0075</td>
      <td>93.4941</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1566</th>
      <td>2008-10-17 06:07:00</td>
      <td>2944.92</td>
      <td>2450.76</td>
      <td>2195.4444</td>
      <td>2914.1792</td>
      <td>1.5978</td>
      <td>100.0</td>
      <td>85.1011</td>
      <td>0.1235</td>
      <td>NaN</td>
      <td>...</td>
      <td>137.7844</td>
      <td>0.4987</td>
      <td>0.0181</td>
      <td>0.0040</td>
      <td>3.6275</td>
      <td>0.0117</td>
      <td>0.0162</td>
      <td>0.0045</td>
      <td>137.7844</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 592 columns</p>
</div>



생선된 1개의 반도체당 589개의 센서가 데이터를 수집하며 최종적으로 pass/fail 여부를 기록해놓은 정보입니다.
자 5행의 데이터로부터 어떤 재미있는 생각들을 할 수 있을까요? 제가 떠올린 생각들을 아무렇게나 적어보겠습니다.

1. 아니 최종 합/불을 가려내는데 589개 센서나 필요해?
2. 각 데이터는 뭘 의미하는거야?
3. 총 1566행과 592개의 열로 이루어진 데이터프레임이군.
3. 결측치도 있네?
4. 시간 순서대로 측정된 데이터 구나? 
5. 위 데이터를 바탕으로 회귀/분류 모델을 통해 pass/fail 예측을 할 수 있겠는데?

이 정도로 될 수 있겠네요.

### 그 중 핵심적이라고 판단한 질문은 "제가 이 데이터를 받아본 순간 들었던 생각은 왜 이렇게 센서가 많아?" 였습니다.

영향력있는 데이터를 수집하는 센서들을 추려낸다면 관리비도 적게 들고, 공정팀이 이를 관리하기도 더 편하겠다는 생각으로 이어진 것이죠.

### 그러면 반도체 합/불 판단에 중요한 역할을 하는 센서들은 어떻게 추려낼까요?

어떠한 지표가 있다면 좋겠지만 지금 주어진 정보도 전무하고, 더군다나 반도체 생산에 대해서 아는바도 많이 없습니다.(대학교 시절 얼핏배운 8대 공정정도..?)
그래서 로지스틱 회귀를 사용해서 분류모델을 만들고, 거기에 나온 영향인자를 순서대로 판단해볼까 합니다. 

** 로지스틱 회귀분석에 대해서 미리 학습하실 분들은 다음 링크를 참고해주세요!
https://dprdmr.tistory.com/34 

#### 회귀분석을 진행하기 위해서는 일단 데이터의 결측치 처리를 해주어야합니다.
다음은 결측치 처리 과정입니다.

https://rfriend.tistory.com/402 (참고: 결측치 평균 대체방법)


```python
df.isnull().sum()
```




    Time          0
    0             6
    1             7
    2            14
    3            14
                 ..
    586           1
    587           1
    588           1
    589           1
    Pass/Fail     0
    Length: 592, dtype: int64



#### 결측치를 대체하는 방법으로 여러 가지 방법이 있지만 일단은 평균으로 대체해보겠습니다.
#### 중위값, 0, 이전값, 바로 다음의 값 대체 등 다양한 방법이 있습니다.


```python
import numpy as np

df= df.fillna(df.mean())
df.isnull().sum()
```

    /var/folders/q8/p_lbgb2j2b37x4b4dp3t5m0c0000gn/T/ipykernel_1946/2538535510.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
      df= df.fillna(df.mean())





    Time         0
    0            0
    1            0
    2            0
    3            0
                ..
    586          0
    587          0
    588          0
    589          0
    Pass/Fail    0
    Length: 592, dtype: int64




```python
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>581</th>
      <th>582</th>
      <th>583</th>
      <th>584</th>
      <th>585</th>
      <th>586</th>
      <th>587</th>
      <th>588</th>
      <th>589</th>
      <th>Pass/Fail</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2008-07-19 11:55:00</td>
      <td>3030.93</td>
      <td>2564.00</td>
      <td>2187.7333</td>
      <td>1411.1265</td>
      <td>1.3602</td>
      <td>100.0</td>
      <td>97.6133</td>
      <td>0.1242</td>
      <td>1.500500</td>
      <td>...</td>
      <td>97.934373</td>
      <td>0.5005</td>
      <td>0.0118</td>
      <td>0.0035</td>
      <td>2.3630</td>
      <td>0.021458</td>
      <td>0.016475</td>
      <td>0.005283</td>
      <td>99.670066</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2008-07-19 12:32:00</td>
      <td>3095.78</td>
      <td>2465.14</td>
      <td>2230.4222</td>
      <td>1463.6606</td>
      <td>0.8294</td>
      <td>100.0</td>
      <td>102.3433</td>
      <td>0.1247</td>
      <td>1.496600</td>
      <td>...</td>
      <td>208.204500</td>
      <td>0.5019</td>
      <td>0.0223</td>
      <td>0.0055</td>
      <td>4.4447</td>
      <td>0.009600</td>
      <td>0.020100</td>
      <td>0.006000</td>
      <td>208.204500</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2008-07-19 13:17:00</td>
      <td>2932.61</td>
      <td>2559.94</td>
      <td>2186.4111</td>
      <td>1698.0172</td>
      <td>1.5102</td>
      <td>100.0</td>
      <td>95.4878</td>
      <td>0.1241</td>
      <td>1.443600</td>
      <td>...</td>
      <td>82.860200</td>
      <td>0.4958</td>
      <td>0.0157</td>
      <td>0.0039</td>
      <td>3.1745</td>
      <td>0.058400</td>
      <td>0.048400</td>
      <td>0.014800</td>
      <td>82.860200</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2008-07-19 14:43:00</td>
      <td>2988.72</td>
      <td>2479.90</td>
      <td>2199.0333</td>
      <td>909.7926</td>
      <td>1.3204</td>
      <td>100.0</td>
      <td>104.2367</td>
      <td>0.1217</td>
      <td>1.488200</td>
      <td>...</td>
      <td>73.843200</td>
      <td>0.4990</td>
      <td>0.0103</td>
      <td>0.0025</td>
      <td>2.0544</td>
      <td>0.020200</td>
      <td>0.014900</td>
      <td>0.004400</td>
      <td>73.843200</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2008-07-19 15:22:00</td>
      <td>3032.24</td>
      <td>2502.87</td>
      <td>2233.3667</td>
      <td>1326.5200</td>
      <td>1.5334</td>
      <td>100.0</td>
      <td>100.3967</td>
      <td>0.1235</td>
      <td>1.503100</td>
      <td>...</td>
      <td>97.934373</td>
      <td>0.4800</td>
      <td>0.4766</td>
      <td>0.1045</td>
      <td>99.3032</td>
      <td>0.020200</td>
      <td>0.014900</td>
      <td>0.004400</td>
      <td>73.843200</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1562</th>
      <td>2008-10-16 15:13:00</td>
      <td>2899.41</td>
      <td>2464.36</td>
      <td>2179.7333</td>
      <td>3085.3781</td>
      <td>1.4843</td>
      <td>100.0</td>
      <td>82.2467</td>
      <td>0.1248</td>
      <td>1.342400</td>
      <td>...</td>
      <td>203.172000</td>
      <td>0.4988</td>
      <td>0.0143</td>
      <td>0.0039</td>
      <td>2.8669</td>
      <td>0.006800</td>
      <td>0.013800</td>
      <td>0.004700</td>
      <td>203.172000</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1563</th>
      <td>2008-10-16 20:49:00</td>
      <td>3052.31</td>
      <td>2522.55</td>
      <td>2198.5667</td>
      <td>1124.6595</td>
      <td>0.8763</td>
      <td>100.0</td>
      <td>98.4689</td>
      <td>0.1205</td>
      <td>1.433300</td>
      <td>...</td>
      <td>97.934373</td>
      <td>0.4975</td>
      <td>0.0131</td>
      <td>0.0036</td>
      <td>2.6238</td>
      <td>0.006800</td>
      <td>0.013800</td>
      <td>0.004700</td>
      <td>203.172000</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1564</th>
      <td>2008-10-17 05:26:00</td>
      <td>2978.81</td>
      <td>2379.78</td>
      <td>2206.3000</td>
      <td>1110.4967</td>
      <td>0.8236</td>
      <td>100.0</td>
      <td>99.4122</td>
      <td>0.1208</td>
      <td>1.462862</td>
      <td>...</td>
      <td>43.523100</td>
      <td>0.4987</td>
      <td>0.0153</td>
      <td>0.0041</td>
      <td>3.0590</td>
      <td>0.019700</td>
      <td>0.008600</td>
      <td>0.002500</td>
      <td>43.523100</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1565</th>
      <td>2008-10-17 06:01:00</td>
      <td>2894.92</td>
      <td>2532.01</td>
      <td>2177.0333</td>
      <td>1183.7287</td>
      <td>1.5726</td>
      <td>100.0</td>
      <td>98.7978</td>
      <td>0.1213</td>
      <td>1.462200</td>
      <td>...</td>
      <td>93.494100</td>
      <td>0.5004</td>
      <td>0.0178</td>
      <td>0.0038</td>
      <td>3.5662</td>
      <td>0.026200</td>
      <td>0.024500</td>
      <td>0.007500</td>
      <td>93.494100</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1566</th>
      <td>2008-10-17 06:07:00</td>
      <td>2944.92</td>
      <td>2450.76</td>
      <td>2195.4444</td>
      <td>2914.1792</td>
      <td>1.5978</td>
      <td>100.0</td>
      <td>85.1011</td>
      <td>0.1235</td>
      <td>1.462862</td>
      <td>...</td>
      <td>137.784400</td>
      <td>0.4987</td>
      <td>0.0181</td>
      <td>0.0040</td>
      <td>3.6275</td>
      <td>0.011700</td>
      <td>0.016200</td>
      <td>0.004500</td>
      <td>137.784400</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
<p>1567 rows × 592 columns</p>
</div>



데이터의 결측치가 모두 없어진것을 확인할 수 있습니다.

#### 아래는 로지스틱 회귀 진행 과정입니다. 

### 1. 학습데이터/ 테스트 데이터 추출


```python
df= df.drop(columns=["Time"],axis=1) # 시간열은 필요없으니 없애줍니다. 
x = df.drop(columns= ['Pass/Fail'], axis=1)
y = df['Pass/Fail']
```


```python
# 사이킷런 패키지에는 테스트데이터와 학습데이터를 분리시켜주는 클래스가 있습니다. 
# 이 클래스에 원래 데이터와 테스트 데이터 비율을 입력시켜주면 됩니다. 

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)
```

각 센서별 측정하는 데이터의 값이 매우 다르므로 정규화를 시켜주겠습니다. 

#### 정규화란 데이터의 scale을 맞춰주는 작업입니다.
https://hleecaster.com/ml-normalization-concept/


```python
from sklearn.preprocessing import StandardScaler # 사이킷런 전처리 모듈에서 스탠다드 스케일러 클래스를 가져오겠습니다. 
sc = StandardScaler()

x_train = sc.fit_transform(X_train) # 스탠다드 스케일러 클래스의 fit_transform 모듈에 x값 넣어서 설정해주기 
x_test = sc.transform(X_test) 

```

### train data에는 fit_transform이고 test data는 transform하는 이유는 ??

train data로부터 학습된 mean값과 variance값을 test data에 적용하기 위해 transform() 메서드 사용 
즉 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환해야 하는 것입니다.  
학습 데이터에서 Scale된 데이터를 기반으로 Classifier가 학습이 되었기 때문에 이렇게 학습된 Classifier가 예측을 할 때에도 학습 데이터의 Scale 기준으로 테스트 데이터를 변환 한 뒤 predict 해야 합니다.


```python
from sklearn.linear_model import LogisticRegression #사이킷런 패키지로부터 로지스틱회귀 클래스 가져오기
model = LogisticRegression(max_iter=5000) # 로지스틱회귀 클래스 내부에 이터레이션 정보를 입력해주는것. 

model.fit(x_train, Y_train) # fit 함수로 데이터를 학습시킵니다.

# score 함수를 사용하여 모델의 성능을 확인합니다. 
print(model.score(x_train, Y_train))
print(model.score(x_test, Y_test))
```

    0.9904229848363927
    0.89171974522293



```python
sorted(enumerate(abs_coef))
```




    [(0, 0.15513549218757758),
     (1, 0.08244760069141856),
     (2, 0.38515803438770607),
     (3, 0.03276394898939544),
     (4, 0.0029632376542025967),
     (5, 0.0),
     (6, 0.03432594918436178),
     (7, 0.14678143554871492),
     (8, 0.06438742338366002),
     (9, 0.35114586124428654),
     (10, 0.38039372402040267),
     (11, 0.05289625099879434),
     (12, 0.021456918219606002),
     (13, 0.0),
     (14, 0.5336290071488653),
     (15, 0.35510944837123565),
     (16, 0.43729366110877615),
     (17, 0.06394128847321474),
     (18, 0.39699751023275404),
     (19, 0.1836421459291836),
     (20, 0.1641335687730198),
     (21, 0.46919641131377293),
     (22, 0.5490947860818542),
     (23, 0.028201956154209066),
     (24, 0.08559402110428052),
     (25, 0.033393992657176894),
     (26, 0.37063070210914906),
     (27, 0.06409229348656507),
     (28, 0.2896710000849176),
     (29, 0.18977977069886143),
     (30, 0.519600455783872),
     (31, 0.2841038284120025),
     (32, 0.7613718848173825),
     (33, 0.04830954207298269),
     (34, 0.28737875419644104),
     (35, 0.3080833976611905),
     (36, 0.28742300356379524),
     (37, 0.1966820831266193),
     (38, 0.17445570196220983),
     (39, 0.0751919884458986),
     (40, 0.38672537754042136),
     (41, 0.47999824832493027),
     (42, 0.0),
     (43, 0.2515399204697149),
     (44, 0.38058581349057563),
     (45, 0.31799279652807855),
     (46, 0.35554637002075534),
     (47, 0.1805631803072397),
     (48, 0.32091665257158203),
     (49, 0.0),
     (50, 0.30848587532295346),
     (51, 0.03428215474839904),
     (52, 0.0),
     (53, 0.704725751902416),
     (54, 0.4828223041402164),
     (55, 0.2108165469702491),
     (56, 1.462265167912056),
     (57, 0.40956113473994604),
     (58, 0.23215790643031978),
     (59, 1.7412463767829234),
     (60, 0.4992698427814193),
     (61, 1.1194770555674547),
     (62, 0.5776239842973945),
     (63, 0.20381148328132648),
     (64, 0.6231533375133319),
     (65, 0.5227419590894692),
     (66, 0.5946229342712432),
     (67, 0.6805385796791596),
     (68, 0.3485614744236892),
     (69, 0.0),
     (70, 0.3877329566598589),
     (71, 0.5864647691122267),
     (72, 0.5626720030810017),
     (73, 0.4945874090019129),
     (74, 0.0018086922071572952),
     (75, 0.3775041246910244),
     (76, 0.18587152107902588),
     (77, 0.7526715605948433),
     (78, 0.23113310800498232),
     (79, 0.4417801935365404),
     (80, 0.3615894274964721),
     (81, 0.22632750754017558),
     (82, 0.00508837940271459),
     (83, 0.289449112502766),
     (84, 0.019708120391534228),
     (85, 0.004536585479709444),
     (86, 0.5719008082294345),
     (87, 0.01506152143878261),
     (88, 0.16685951085104797),
     (89, 0.5183170061354769),
     (90, 0.15554596887745234),
     (91, 0.14662645164718208),
     (92, 0.29204968121601105),
     (93, 0.06267926965058178),
     (94, 0.1905469023503626),
     (95, 0.17074468128054032),
     (96, 0.3112126594609132),
     (97, 0.0),
     (98, 0.008587064448633734),
     (99, 0.5087556312244488),
     (100, 0.1034921706801666),
     (101, 0.4222380854825961),
     (102, 0.5125207908298802),
     (103, 0.2575842405077002),
     (104, 0.1595623655428237),
     (105, 0.4504817511348996),
     (106, 0.13945354930532144),
     (107, 0.03141089427327503),
     (108, 0.5782405731168334),
     (109, 0.2985646926077963),
     (110, 0.02995183492935342),
     (111, 0.6312643794179097),
     (112, 0.043381517852203996),
     (113, 0.758629394982237),
     (114, 0.2166894231149211),
     (115, 0.43396804425244545),
     (116, 0.0820244850368418),
     (117, 0.45724150603454583),
     (118, 0.11693673973581668),
     (119, 0.4122519317657745),
     (120, 0.015534128826083245),
     (121, 0.21275048962354176),
     (122, 0.5394828312795418),
     (123, 0.05875171457764719),
     (124, 0.4317532611985308),
     (125, 0.21941949187171617),
     (126, 0.23292514087726085),
     (127, 0.35053538075837304),
     (128, 0.10578504418086784),
     (129, 1.0734549400715971),
     (130, 0.30409679510034954),
     (131, 0.6883553851086116),
     (132, 1.4196386257364162),
     (133, 0.3226637253505929),
     (134, 0.33068640005914113),
     (135, 0.19232421793484383),
     (136, 0.06281623671015964),
     (137, 0.2450280761249139),
     (138, 0.5565475293777342),
     (139, 0.0036010491869783233),
     (140, 0.0031559468416389384),
     (141, 0.0),
     (142, 0.45731474869375566),
     (143, 0.2374417587258301),
     (144, 0.27517599527286785),
     (145, 0.4685374904675969),
     (146, 0.22432468283399673),
     (147, 0.05575072020957218),
     (148, 0.17781454834322655),
     (149, 0.0),
     (150, 0.01441445072344534),
     (151, 0.47785410570024506),
     (152, 0.12940862123347452),
     (153, 0.20516089854647596),
     (154, 0.057613572961429424),
     (155, 0.6060344539222975),
     (156, 0.0422255532557575),
     (157, 0.41277490425368346),
     (158, 0.38534643320233697),
     (159, 0.6000462552924094),
     (160, 0.43517013585904274),
     (161, 0.23396645361726767),
     (162, 0.3248111494974139),
     (163, 0.018938311893836653),
     (164, 0.30951384220890904),
     (165, 0.4024190308248551),
     (166, 0.21662571766716204),
     (167, 0.0507242516424671),
     (168, 0.002336159461001573),
     (169, 0.012508588083562212),
     (170, 0.03939576903235794),
     (171, 0.08183935445049893),
     (172, 0.3961678664337221),
     (173, 0.21400232699390231),
     (174, 0.399457741730503),
     (175, 0.36885708304993087),
     (176, 0.2737299571312197),
     (177, 0.3344527383241221),
     (178, 0.0),
     (179, 0.0),
     (180, 0.19080510644928725),
     (181, 0.12031768242393955),
     (182, 0.22846124701816148),
     (183, 0.23562323333323448),
     (184, 0.0632317991288378),
     (185, 0.13709393671496176),
     (186, 0.0),
     (187, 0.24530242825173645),
     (188, 0.6143897777569126),
     (189, 0.0),
     (190, 0.0),
     (191, 0.0),
     (192, 0.0),
     (193, 0.0),
     (194, 0.0),
     (195, 0.14622036312966094),
     (196, 0.034425386702350194),
     (197, 0.42494706200159443),
     (198, 0.06865293055341591),
     (199, 0.14461410117313267),
     (200, 0.37966853324329425),
     (201, 0.7199088807246814),
     (202, 0.19354534721332248),
     (203, 0.2770885851972884),
     (204, 0.5259645874715458),
     (205, 0.5141552754946116),
     (206, 0.0018086922071572985),
     (207, 0.07203418560616778),
     (208, 0.2744043168265644),
     (209, 0.0018086922071573024),
     (210, 0.04878189301829573),
     (211, 0.3609948640590023),
     (212, 0.5182249854454041),
     (213, 0.013350060731602054),
     (214, 0.3105334887697362),
     (215, 0.24405794698734729),
     (216, 0.46372704499496625),
     (217, 0.15179189172411367),
     (218, 0.13422535310548048),
     (219, 0.024882939369932962),
     (220, 0.1410654162212853),
     (221, 0.6202819246170533),
     (222, 0.09907480965401018),
     (223, 0.21682064705272422),
     (224, 0.1944197551637516),
     (225, 0.13924800792179168),
     (226, 0.0),
     (227, 0.2559466511265515),
     (228, 0.34791144815909264),
     (229, 0.0),
     (230, 0.0),
     (231, 0.0),
     (232, 0.0),
     (233, 0.0),
     (234, 0.0),
     (235, 0.0),
     (236, 0.0),
     (237, 0.0),
     (238, 0.4167235939601524),
     (239, 0.795229906055786),
     (240, 0.0),
     (241, 0.0),
     (242, 0.0),
     (243, 0.0),
     (244, 0.09699817253327776),
     (245, 0.07389393506729237),
     (246, 0.018545151083372277),
     (247, 0.04129791584069872),
     (248, 0.6091763916498476),
     (249, 0.6366298671959523),
     (250, 0.18551695634246604),
     (251, 0.3033804030246653),
     (252, 0.017407960417993466),
     (253, 0.03292377172750731),
     (254, 0.023158900169548535),
     (255, 0.09189223048206668),
     (256, 0.0),
     (257, 0.0),
     (258, 0.0),
     (259, 0.0),
     (260, 0.0),
     (261, 0.0),
     (262, 0.0),
     (263, 0.0),
     (264, 0.0),
     (265, 0.0),
     (266, 0.0),
     (267, 0.3101876389312072),
     (268, 0.5061392364645939),
     (269, 0.12250544271210083),
     (270, 0.14880249658758743),
     (271, 0.2794029526295718),
     (272, 0.1402370903486572),
     (273, 0.4052278526497681),
     (274, 0.09242584499363302),
     (275, 0.0030018440509429075),
     (276, 0.0),
     (277, 0.5198061804319404),
     (278, 0.12338296031934134),
     (279, 0.44503165004334144),
     (280, 0.1795852831607955),
     (281, 0.23002632799653291),
     (282, 0.12894837161341638),
     (283, 0.2636512837624748),
     (284, 0.0),
     (285, 0.13865198134497864),
     (286, 0.4187082240966804),
     (287, 0.14384482531698667),
     (288, 0.14071357382909963),
     (289, 0.2530263778601353),
     (290, 0.2884863519957377),
     (291, 0.26763772543428105),
     (292, 0.16182535751041677),
     (293, 0.06698461703219737),
     (294, 0.21279416070720175),
     (295, 0.3013481919054902),
     (296, 0.05208103625250423),
     (297, 0.21992002993752796),
     (298, 0.044485792281805964),
     (299, 0.23477846478172074),
     (300, 0.37425004263463474),
     (301, 0.13845447346966577),
     (302, 0.4603418186040845),
     (303, 0.1652988759661308),
     (304, 0.4710392716095345),
     (305, 0.20520023191127543),
     (306, 0.03991073505983273),
     (307, 0.4286755477043982),
     (308, 0.1123109720358092),
     (309, 0.42718338630328445),
     (310, 0.7053604440229597),
     (311, 0.7544199862452294),
     (312, 0.3985025541766799),
     (313, 0.0),
     (314, 0.0),
     (315, 0.0),
     (316, 0.24888884458452687),
     (317, 0.39111722556704426),
     (318, 0.21191837230801716),
     (319, 0.3680798159571066),
     (320, 0.2571887777812319),
     (321, 0.14780908169376386),
     (322, 0.0),
     (323, 0.14455524289694344),
     (324, 0.3783838824239375),
     (325, 0.0),
     (326, 0.0),
     (327, 0.0),
     (328, 0.0),
     (329, 0.0),
     (330, 0.0),
     (331, 0.08247596591120923),
     (332, 0.3388666032551703),
     (333, 0.5154485001801059),
     (334, 0.3742691157089699),
     (335, 0.1327414220690731),
     (336, 0.11744885090219734),
     (337, 0.45835552138493857),
     (338, 0.2756262746385087),
     (339, 0.40145923027843683),
     (340, 0.5397266597430757),
     (341, 0.08471315522468144),
     (342, 0.0018086922071573093),
     (343, 0.023204474655604143),
     (344, 0.2472215866858483),
     (345, 0.02716945714874847),
     (346, 0.07295579331301567),
     (347, 0.0018086922071573037),
     (348, 0.25244376685679193),
     (349, 0.4568343930531434),
     (350, 0.36686583095311903),
     (351, 0.17909718051782314),
     (352, 0.026821449165030178),
     (353, 0.044830403363800085),
     (354, 0.354386818201431),
     (355, 0.13812009260361907),
     (356, 0.2684614154920786),
     (357, 0.2089171102826986),
     (358, 0.584515819543326),
     (359, 0.5501138954184857),
     (360, 0.04984606181890261),
     (361, 0.5151078367882076),
     (362, 0.14427981577395532),
     (363, 0.06439487028604096),
     (364, 0.0),
     (365, 0.5398043328511832),
     (366, 0.09404589108604935),
     (367, 0.5130050605785907),
     (368, 0.019545542872415174),
     (369, 0.0),
     (370, 0.0),
     (371, 0.0),
     (372, 0.0),
     (373, 0.0),
     (374, 0.0),
     (375, 0.0),
     (376, 0.21680438949190667),
     (377, 0.76371822310354),
     (378, 0.0),
     (379, 0.0),
     (380, 0.0),
     (381, 0.0),
     (382, 0.10824918335587723),
     (383, 0.09734544563749639),
     (384, 0.1503611132699515),
     (385, 0.11219407532269633),
     (386, 0.4266313253300777),
     (387, 0.6180092391201198),
     (388, 0.933136688118364),
     (389, 0.3148403398613767),
     (390, 0.03211409123120934),
     (391, 0.1319033500698396),
     (392, 0.07336380245595997),
     (393, 0.02219765976616521),
     (394, 0.0),
     (395, 0.0),
     (396, 0.0),
     (397, 0.0),
     (398, 0.0),
     (399, 0.0),
     (400, 0.0),
     (401, 0.0),
     (402, 0.0),
     (403, 0.0),
     (404, 0.0),
     (405, 0.09317984639910627),
     (406, 0.3947700572296368),
     (407, 0.07672110405767232),
     (408, 0.19699128599624197),
     (409, 0.10739662040904924),
     (410, 0.31889942418858885),
     (411, 0.4742949609226812),
     (412, 0.05458989218116269),
     (413, 0.6384718677328706),
     (414, 0.0),
     (415, 0.4776917808796531),
     (416, 0.19574449788842857),
     (417, 0.008295778732943767),
     (418, 0.2694840384620993),
     (419, 0.15594476008809396),
     (420, 0.023057463616304962),
     (421, 0.17316980043947888),
     (422, 0.0),
     (423, 0.6619262924528333),
     (424, 0.539161530321124),
     (425, 0.5408231354701941),
     (426, 0.20276444334343022),
     (427, 0.021074777105382327),
     (428, 0.5166062506033262),
     (429, 0.02187427649640466),
     (430, 0.1072321240280004),
     (431, 0.045307699512030175),
     (432, 0.530258569701804),
     (433, 0.5042620002209561),
     (434, 0.25740414625322094),
     (435, 0.3353491105355026),
     (436, 0.4411016496331792),
     (437, 0.3329962476764631),
     (438, 0.2429541238173458),
     (439, 0.58579499149446),
     (440, 0.12910567202913567),
     (441, 0.04127452332158748),
     (442, 0.18941980313092777),
     (443, 0.3954757828304221),
     (444, 0.21914870567425),
     (445, 0.39042033024951533),
     (446, 0.3668181662865544),
     (447, 0.2968917955882978),
     (448, 0.3396150194730878),
     (449, 0.0),
     (450, 0.0),
     (451, 0.0),
     (452, 0.15004556885454923),
     (453, 0.06348518366230678),
     (454, 0.42895522353076637),
     (455, 0.23182106036580533),
     (456, 0.17597053223832917),
     (457, 0.25905458912512935),
     (458, 0.0),
     (459, 0.250398317605092),
     (460, 0.831539019316501),
     (461, 0.0),
     (462, 0.0),
     (463, 0.0),
     (464, 0.0),
     (465, 0.0),
     (466, 0.0),
     (467, 0.16705752874570703),
     (468, 0.16753865251635944),
     (469, 0.3265877977342755),
     (470, 0.01502824495040069),
     (471, 0.5958244534046266),
     (472, 0.2196284807939735),
     (473, 0.07386385994444171),
     (474, 0.09543411344434673),
     (475, 0.2512432026053963),
     (476, 0.009518114432227187),
     (477, 0.2840514998419853),
     (478, 0.0018086922071572943),
     (479, 0.03482602436356802),
     (480, 0.15706104922585262),
     (481, 0.0),
     (482, 0.027560066633021997),
     (483, 0.1769377723239189),
     (484, 0.45709668467345804),
     (485, 0.2899423605540597),
     (486, 0.4374641219459158),
     (487, 0.5703727739792236),
     (488, 0.5060120063126075),
     (489, 0.19519721560500414),
     (490, 0.05003718184995286),
     (491, 0.23239935986492982),
     (492, 0.15437662099920002),
     (493, 0.6581186241137266),
     (494, 0.043488266400192314),
     (495, 0.3479935667547204),
     (496, 0.2544084308207974),
     (497, 0.18683482326193313),
     (498, 0.0),
     (499, 0.15301262064952495),
     (500, 0.32174430069897303),
     (501, 0.0),
     (502, 0.0),
     (503, 0.0),
     (504, 0.0),
     (505, 0.0),
     (506, 0.0),
     (507, 0.0),
     (508, 0.0),
     (509, 0.0),
     (510, 0.16878094570579025),
     (511, 0.7835218974138737),
     (512, 0.0),
     (513, 0.0),
     (514, 0.0),
     (515, 0.0),
     (516, 0.09536834372535626),
     (517, 0.07490624856982153),
     (518, 0.023565308940799678),
     (519, 0.03550352773056204),
     (520, 0.5679634857894499),
     (521, 0.02433994344374502),
     (522, 0.1368011146565932),
     (523, 0.31366832974436637),
     (524, 0.08317917175424974),
     (525, 0.0649015121737076),
     (526, 0.1104800263075933),
     (527, 0.044374146133754476),
     (528, 0.0),
     (529, 0.0),
     (530, 0.0),
     (531, 0.0),
     (532, 0.0),
     (533, 0.0),
     (534, 0.0),
     (535, 0.0),
     (536, 0.0),
     (537, 0.0),
     (538, 0.0),
     (539, 0.3057312995305458),
     (540, 0.5015836047444194),
     (541, 0.29160858770740516),
     (542, 0.09819385097840154),
     (543, 0.5390068601719503),
     (544, 0.4040429656627689),
     (545, 0.47663983452704345),
     (546, 0.11927288697199134),
     (547, 0.6723739351061275),
     (548, 0.22024631277358098),
     (549, 0.3411017395668777),
     (550, 0.2564761671270287),
     (551, 0.3084961568038682),
     (552, 0.07507129748745346),
     (553, 0.09577991458787663),
     (554, 0.2829974643369144),
     (555, 0.04266126517838166),
     (556, 0.3328239681295162),
     (557, 0.3582542580713981),
     (558, 0.12942318958964935),
     (559, 0.11050710482961011),
     (560, 0.06874183883509948),
     (561, 0.0656423451573446),
     (562, 0.27227311385400027),
     (563, 0.7242851864495258),
     (564, 0.3568201960031162),
     (565, 0.15708740248732508),
     (566, 0.7110125627373022),
     (567, 0.5051339106659739),
     (568, 0.24918936551812448),
     (569, 0.18575243204985112),
     (570, 0.17686810803407713),
     (571, 0.3064642595279996),
     (572, 0.15095434502196534),
     (573, 0.2810840057558029),
     (574, 0.15989194397603745),
     (575, 0.039568268616846154),
     (576, 0.06339946232571682),
     (577, 0.5281238820515909),
     (578, 0.253698097068498),
     (579, 0.2528452409773682),
     (580, 0.08948250762695455),
     (581, 0.605586445931261),
     (582, 0.46943042347843883),
     (583, 0.04429576363722852),
     (584, 0.24144368081066697),
     (585, 0.04032627246309173),
     (586, 0.044514915207862896),
     (587, 0.12416524397448919),
     (588, 0.09466490092455843),
     (589, 0.24048669798866487)]



#### 학습데이터로 만든 모델은 98.8 % 정확도를, 테스트 데이터에는 87.9% 정확도를 보임을 알 수 있습니다.

원래 우리가 하려고했던 목적은 중요 변수를 찾아내는 것이었습니다. 따라서 각 센서가 얼마나 이 정확도에 영향을 미쳤는지 보겠습니다.


```python
import matplotlib.pyplot as plt

abs_coef = np.abs(model.coef_).ravel() # 모델의 회귀계수들을 넘파이를 사용하여 절대값으로 만들고 1차원 배열화합니다.

data = [str(i[0]) for i in sorted(enumerate(abs_coef), key=lambda x:x[1], reverse=True)]



```


    
![png](output_24_0.png)
    


enumerate() 함수는 기본적으로 인덱스와 원소로 이루어진 튜플을 만들어줍니다.  
sorted()함수는 매개변수로 들어온 데이터를 새롭게 정렬된 리스트로 만들어서 반환해주는 함수입니다.   
data는 enumerate로 만들어진 튜플에서 첫번째 원소 즉, 센서의 넘버를 뽑아서 만든 리스트입니다.  
sorted를 사용하는데, 사용 기준은 abs_coefficient 즉, 선형회귀 독립변수의 가중치를 바탕으로 높은 순위로 나열된 것입니다.   
따라서 data는 선형회귀 독립변수 중 중요도가 높은 순서대로 모아놓은 리스트입니다.


```python
enumerate(abs_coef) 
```




    <enumerate at 0x7f7ce8ebfac0>



위와 같이 enumerate 자체로는 데이터를 반환하지 않습니다.(함수 코드에 return 값으로 지정이 안되어잇는거겟죠?)


```python
plt.bar(data[:20], sorted(abs_coef, reverse=True)[:20])

plt.rcParams['figure.figsize'] = (15, 10)
plt.xlabel('Features')
plt.ylabel('Weight absolute values')
plt.show()
```


    
![png](output_28_0.png)
    


### 정답에 영향을 미치는 상위 20개 센서들의 값이 다음이 됨을 알 수 있습니다.

그렇다면 이들 값만 사용해서 예측을 하면 결과는 어떻게 될까요??


```python
df_new=df[['59','56','132','61','129','388','460','239','511','377','32','113','311','77','563','201','566','310','53','131','Pass/Fail']]
```


```python
x = df_new.drop(columns= ['Pass/Fail'], axis=1)
y = df_new['Pass/Fail']

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler # 사이킷런 전처리 모듈에서 스탠다드 스케일러 클래스를 가져오겠습니다. 
sc = StandardScaler()

x_train = sc.fit_transform(X_train) # 스탠다드 스케일러 클래스의 fit_transform 모듈에 x값 넣어서 설정해주기 
x_test = sc.transform(X_test) 

from sklearn.linear_model import LogisticRegression #사이킷런 패키지로부터 로지스틱회귀 클래스 가져오기
model = LogisticRegression(max_iter=5000) # 로지스틱회귀 클래스 내부에 이터레이션 정보를 입력해주는것. 

model.fit(x_train, Y_train) # fit 함수로 데이터를 학습시킵니다.

# score 함수를 사용하여 모델의 성능을 확인합니다. 
print(model.score(x_train, Y_train))
print(model.score(x_test, Y_test))
```

    0.9265762170790104
    0.9554140127388535


train data의 예측 성능은 줄었지만 test data 예측성능이 좋아진 것을 볼 수 있습니다.


### 본 프로젝트는 앨리스 코딩 온라인 AI 교육 실습 내용을 기반으로 작성되었습니다.


